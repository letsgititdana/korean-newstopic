{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일별 분류별 신문게재기사 페이지에서 URL 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, URL을 조작해 과거 뉴스의 링크를 얻는 과정을 자동화하기 위해 다음 뉴스목록 URL의 구성요소를 분석했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">https://news.daum.net/breakingnews/<span style=\"color:magenta\">politics</span>?<span style=\"color:forestgreen\">page=1</span>&<span style=\"color:dodgerblue\">regDate=20200830</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제목과 사진만 있어서 본문 내용이 없는 기사나 라디오 인터뷰 형식으로 작성된 기사들을 제외하기 위해 출처가 종합지인 기사들만 추렸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_publishers = ['한국일보','문화일보','동아일보','서울신문','세계일보','경향신문','국민일보','중앙일보','한겨레','조선일보']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고로, 선택할 수 있는 <span style=\"color:magenta\">기사 분류</span>의 목록은 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_categories = ('politics', 'society', 'economic', 'culture', 'entertain', 'digital', 'editorial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래서 만약 한 <span style=\"color:magenta\">분류</span>가 주어졌을 때, 아래와 같은 함수를 통해 형식에 맞는 URL을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pageurl(category, pagenum, date):\n",
    "    \"\"\"\n",
    "    Function to return formatted URL given 3 input parameters\n",
    "    \"\"\"\n",
    "    assert category in valid_categories, 'Invalid input category'\n",
    "    assert len(str(date)) == 8, 'Invalid input date format'\n",
    "    return f\"https://news.daum.net/breakingnews/{category}?page={pagenum}&regDate={date}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형식에 맞는 URL이 유효한 URL이 되려면 <span style=\"color:forestgreen\">페이지 번호</span>와 <span style=\"color:dodgerblue\">날짜</span>가 유효해야 한다. 날짜는 `datetime`패키지를 사용해 유효한 날짜들의 목록을 얻고, 유효한 페이지 번호들의 목록은 request를 전송해서 얻은 HTML파일에서 기사목록이 조회되지 않을 때까지 pagenum을 하나씩 증가시켜보면 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유효한 날짜 목록 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_formatting(date):\n",
    "    \"\"\"\n",
    "    convert datetime.date object into yyyymmdd format string\n",
    "    \"\"\"\n",
    "    year = str(date.year)\n",
    "    month = str(date.month).rjust(2, '0')\n",
    "    day = str(date.day).rjust(2, '0')\n",
    "    return year + month + day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20200902'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_formatting(datetime.date(*[2020,9,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datelist(start, end):\n",
    "    \"\"\"\n",
    "    Given start, end date in yyyymmdd format, obtain list of dates in between\n",
    "    \"\"\"\n",
    "    assert len(str(start)) == 8, 'start date not in proper format: yyyymmdd'\n",
    "    assert len(str(end)) == 8, 'end date not in proper format: yyyymmdd'\n",
    "    \n",
    "    start = str(start)\n",
    "    end = str(end)\n",
    "    \n",
    "    start_list = [start[:4], start[4:6], start[6:]]\n",
    "    end_list = [end[:4], end[4:6], end[6:]]\n",
    "    \n",
    "    start_date = datetime.date(*list(map(int, start_list)))\n",
    "    end_date = datetime.date(*list(map(int, end_list)))\n",
    "    delta = (end_date - start_date).days\n",
    "    assert delta > 0, 'start date must precede end date'\n",
    "    \n",
    "    date_list = [start_date + datetime.timedelta(d) for d in range(delta+1)]\n",
    "    return list(map(date_formatting, date_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20200830', '20200831', '20200901', '20200902', '20200903']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_datelist(20200830, 20200903)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유효한 페이지 번호 얻어서 유효한 URL들 목록 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs for articles of 20200830 scraped completely\n",
      "URLs for articles of 20200831 scraped completely\n",
      "URLs for articles of 20200901 scraped completely\n"
     ]
    }
   ],
   "source": [
    "urllist = []\n",
    "datelist = get_datelist(20200830, 20200901)\n",
    "\n",
    "for date in datelist:\n",
    "    pagenum = 1\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(make_pageurl('politics', pagenum, date))\n",
    "        parsed = bs(response.text, 'html.parser')\n",
    "        body = parsed.find('div', attrs={'class':'box_etc'})\n",
    "\n",
    "        if body.select('p.txt_none'):\n",
    "            break\n",
    "        else:\n",
    "            articles = body.select('div.cont_thumb')\n",
    "            for tag in articles:\n",
    "                publisher = tag.find('span', attrs={'class':'info_news'}).get_text().split()[0]\n",
    "                if publisher in paper_publishers:\n",
    "                    urllist.append(tag.find('a', attrs={'class':'link_txt'})['href'])\n",
    "\n",
    "            pagenum += 1\n",
    "\n",
    "    print(f\"URLs for articles of {date} scraped completely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1019"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urllist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
