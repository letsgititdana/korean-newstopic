{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.special import digamma\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikitext_sentences(url):\n",
    "    response = requests.get(url)\n",
    "    parsed = BeautifulSoup(response.text, 'html.parser')\n",
    "    return [tag.get_text().strip() for tag in parsed.select('div.mw-parser-output')[0].find_all('p')]\n",
    "\n",
    "\n",
    "def postprocess(paragraphs):\n",
    "    result = []\n",
    "    wrapper = re.compile('[({\\[].{,20}[)}\\]]')\n",
    "    whitespace = re.compile('\\s')\n",
    "    for paragraph in paragraphs:\n",
    "        processed = wrapper.sub('', paragraph)\n",
    "        result.append(whitespace.sub(' ', processed))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 말뭉치 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세 가지 서로 다른 주제의 wikipedia ariticle들의 각 문단들을 하나의 document로 간주해서 가장 기본 형태의 LDA를 구현한 후 이 모형이 세 가지 토픽의 구성 단어들을 효과적으로 찾아내는지 확인한다. 세 article들의 제목은 각각 아래와 같다. 간단한 전처리를 위해 모든 소괄호, 대괄호와 그 안에 포함된 문자열을 삭제했고, `\\n`과 같이 공백을 표현하는 문자열은 ' '로 일괄 변경했다.\n",
    "\n",
    "1. Korea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korea is a region in East Asia; since 1945 it has been divided into what are now two distinct sovereign states: North Korea (officially the \"Democratic People's Republic of Korea\") and South Korea (officially the \"Republic of Korea\"). Korea consists of the Korean Peninsula, Jeju Island, and several minor islands near the peninsula. It is bordered by China to the northwest and Russia to the northeast. It is separated from Japan to the east by the Korea Strait and the Sea of Japan .\n"
     ]
    }
   ],
   "source": [
    "korea = postprocess(get_wikitext_sentences(\"https://en.wikipedia.org/wiki/Korea\"))\n",
    "print(korea[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Aristotle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aristotle  Greek: Ἀριστοτέλης Aristotélēs, pronounced ; 384–322 BC) was a Greek philosopher and polymath during the Classical period in Ancient Greece. Taught by Plato, he was the founder of the Lyceum, the Peripatetic school of philosophy, and the Aristotelian tradition. His writings cover many subjects including physics, biology, zoology, metaphysics, logic, ethics, aesthetics, poetry, theatre, music, rhetoric, psychology, linguistics, economics, politics, and government. Aristotle provided a complex synthesis of the various philosophies existing prior to him. It was above all from his teachings that the West inherited its intellectual lexicon, as well as problems and methods of inquiry. As a result, his philosophy has exerted a unique influence on almost every form of knowledge in the West and it continues to be a subject of contemporary philosophical discussion.\n"
     ]
    }
   ],
   "source": [
    "aristotle = postprocess(get_wikitext_sentences(\"https://en.wikipedia.org/wiki/Aristotle\"))\n",
    "print(aristotle[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google, LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, a search engine, cloud computing, software, and hardware. It is considered one of the Big Four technology companies, alongside Amazon, Apple, and Microsoft.\n"
     ]
    }
   ],
   "source": [
    "google = postprocess(get_wikitext_sentences(\"https://en.wikipedia.org/wiki/Google\"))\n",
    "print(google[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세 부류의 문단들을 하나로 합쳐서 말뭉치를 만들고, 문단들의 순서를 랜덤하게 섞는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = korea + aristotle + google\n",
    "documents = list(np.random.choice(documents, len(documents), replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모형 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 322개의 문서가 있고, 한 iteration마다 전체 문서에서 batchsize만큼의 문서를 샘플링해서 variational parameter들을 업데이트한다. batchsize가 총 문서의 개수와 같으면(즉, 이 예시에서는 322면) 일반적인 LDA와 같은 결과를 얻을 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineLDA:\n",
    "    \n",
    "    \n",
    "    def __init__(self, references, n_documents, n_topics, batchsize):\n",
    "        \"\"\"\n",
    "        references is expected to be one dimensional list\n",
    "        \"\"\"\n",
    "        self.references = np.array(references)\n",
    "        self.D = n_documents\n",
    "        self.K = n_topics\n",
    "        self.S = batchsize\n",
    "        self.vocab2idx = defaultdict(lambda: len(self.vocab2idx))\n",
    "        \n",
    "        \n",
    "    def get_batch_indices(self):\n",
    "        \"\"\"\n",
    "        Operating np.random.choice(range(D), batchsize) becomes expensive when D gets larger.\n",
    "        This function provides faster way to generate random numbers from discrete uniform distribution [0, D)\n",
    "        \"\"\"\n",
    "        return list(map(int, np.random.random((self.S,)) * self.D))\n",
    "        \n",
    "        \n",
    "    def reference_to_wordlist(self, reference):\n",
    "        \"\"\"\n",
    "        (** Function to be distributed **)\n",
    "        Process of converting : reference -> <single document> -> words(list)\n",
    "\n",
    "         1. reference -> document (** domain specific **)\n",
    "             In this simple example, reference is document by itself. \n",
    "             However, this job can be more complicated if, for example, reference is only a URL whose corresponding page contains body text to be scraped.\n",
    "\n",
    "         2. document -> words (** domain specific **)\n",
    "             This example extracts words from English document, so textblob.TextBlob is used.\n",
    "             Depending on the specific language to be analyzed, morpheme analyzer should be changed accordingly.\n",
    "        \"\"\"\n",
    "        # 1. reference -> document\n",
    "        document = reference\n",
    "\n",
    "        # 2. document -> words\n",
    "        wordlist = [tag[0].lower() \n",
    "                    for tag in TextBlob(document).tags \n",
    "                    if tag[1].startswith('NN') and len(tag[0]) > 1]\n",
    "        \n",
    "        # return wordlist\n",
    "        return wordlist\n",
    "    \n",
    "    \n",
    "    def wordlists_to_indexed_docs(self, wordlists):\n",
    "        \"\"\"\n",
    "        Convert <wordlists> into lists of word index. Requires instance variable vocab2idx defined as:\n",
    "        >>> vocab2idx = defaultdict(lambda: len(vocab2idx))\n",
    "        \"\"\"\n",
    "        indexed_docs = []\n",
    "        for wordlist in wordlists:\n",
    "#             print(wordlist)\n",
    "#             print('\\n')\n",
    "            indexed_docs.append([self.vocab2idx[word] for word in wordlist])\n",
    "            \n",
    "        return indexed_docs\n",
    "    \n",
    "    \n",
    "    def batch_call(self):\n",
    "        \"\"\"\n",
    "        Sample D documents(distinct sequence of words) from corpus and convert each as sequence of indices.\n",
    "        \"\"\"\n",
    "        batch_idx = self.get_batch_indices()\n",
    "        minibatch = self.references[batch_idx]\n",
    "    \n",
    "        wordlists = []\n",
    "        for reference in minibatch:\n",
    "#             print(reference + '\\n')\n",
    "            wordlists.append(self.reference_to_wordlist(reference))\n",
    "        \n",
    "        return self.wordlists_to_indexed_docs(wordlists)\n",
    "    \n",
    "    \n",
    "    def estimate(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def idx2vocab(self):\n",
    "        \"\"\"\n",
    "        Create hashmap from each vocaulary's index to vocabulary for result interpretation\n",
    "        \"\"\"\n",
    "        return dict((item[1], item[0]) for item in self.vocab2idx.items())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
