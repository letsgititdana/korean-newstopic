{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.special import digamma\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikitext_sentences(url):\n",
    "    response = requests.get(url)\n",
    "    parsed = BeautifulSoup(response.text, 'html.parser')\n",
    "    return [tag.get_text().strip() for tag in parsed.select('div.mw-parser-output')[0].find_all('p')]\n",
    "\n",
    "\n",
    "def postprocess(paragraphs):\n",
    "    result = []\n",
    "    wrapper = re.compile('[({\\[].{,20}[)}\\]]')\n",
    "    whitespace = re.compile('\\s')\n",
    "    for paragraph in paragraphs:\n",
    "        processed = wrapper.sub('', paragraph)\n",
    "        result.append(whitespace.sub(' ', processed))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 말뭉치 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세 가지 서로 다른 주제의 wikipedia ariticle들의 각 문단들을 하나의 document로 간주해서 가장 기본 형태의 LDA를 구현한 후 이 모형이 세 가지 토픽의 구성 단어들을 효과적으로 찾아내는지 확인한다. 세 article들의 제목은 각각 아래와 같다. 간단한 전처리를 위해 모든 소괄호, 대괄호와 그 안에 포함된 문자열을 삭제했고, `\\n`과 같이 공백을 표현하는 문자열은 ' '로 일괄 변경했다.\n",
    "\n",
    "1. Korea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korea is a region in East Asia; since 1945 it has been divided into what are now two distinct sovereign states: North Korea (officially the \"Democratic People's Republic of Korea\") and South Korea (officially the \"Republic of Korea\"). Korea consists of the Korean Peninsula, Jeju Island, and several minor islands near the peninsula. It is bordered by China to the northwest and Russia to the northeast. It is separated from Japan to the east by the Korea Strait and the Sea of Japan .\n"
     ]
    }
   ],
   "source": [
    "korea = postprocess(get_wikitext_sentences(\"https://en.wikipedia.org/wiki/Korea\"))\n",
    "print(korea[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Aristotle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aristotle  Greek: Ἀριστοτέλης Aristotélēs, pronounced ; 384–322 BC) was a Greek philosopher and polymath during the Classical period in Ancient Greece. Taught by Plato, he was the founder of the Lyceum, the Peripatetic school of philosophy, and the Aristotelian tradition. His writings cover many subjects including physics, biology, zoology, metaphysics, logic, ethics, aesthetics, poetry, theatre, music, rhetoric, psychology, linguistics, economics, politics, and government. Aristotle provided a complex synthesis of the various philosophies existing prior to him. It was above all from his teachings that the West inherited its intellectual lexicon, as well as problems and methods of inquiry. As a result, his philosophy has exerted a unique influence on almost every form of knowledge in the West and it continues to be a subject of contemporary philosophical discussion.\n"
     ]
    }
   ],
   "source": [
    "aristotle = postprocess(get_wikitext_sentences(\"https://en.wikipedia.org/wiki/Aristotle\"))\n",
    "print(aristotle[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google, LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, a search engine, cloud computing, software, and hardware. It is considered one of the Big Four technology companies, alongside Amazon, Apple, and Microsoft.\n"
     ]
    }
   ],
   "source": [
    "google = postprocess(get_wikitext_sentences(\"https://en.wikipedia.org/wiki/Google\"))\n",
    "print(google[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세 부류의 문단들을 하나로 합쳐서 말뭉치를 만들고, 문단들의 순서를 랜덤하게 섞는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = korea + aristotle + google\n",
    "documents = list(np.random.choice(documents, len(documents), replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모형 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 322개의 문서가 있고, 한 iteration마다 전체 문서에서 batchsize만큼의 문서를 샘플링해서 variational parameter들을 업데이트한다. batchsize가 총 문서의 개수와 같으면(즉, 이 예시에서는 322면) 일반적인 LDA와 같은 결과를 얻을 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineLDA:\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 references,\n",
    "                 n_documents,\n",
    "                 maxlen_vocabs,\n",
    "                 n_topics,\n",
    "                 batchsize,\n",
    "                 alpha=0.1, eta=0.01, tau=1, kappa=0.75, tol=1e-4, local_maxiter=100, global_maxiter=1000):\n",
    "        \"\"\"\n",
    "        Aiming truely online LDA. So although D can be exact, but V will be approximate.\n",
    "        \"\"\"\n",
    "        self.references = np.array(references)\n",
    "\n",
    "        self._vocab2idx = defaultdict(lambda: len(self._vocab2idx))\n",
    "        self._D = n_documents\n",
    "        self._V = maxlen_vocabs\n",
    "        self._K = n_topics\n",
    "        self._S = batchsize\n",
    "        self._alpha = alpha\n",
    "        self._eta = eta\n",
    "        self._tau = tau\n",
    "        self._kappa = kappa\n",
    "        self._tol = tol\n",
    "        self._local_maxiter = local_maxiter\n",
    "        self._global_maxiter = global_maxiter\n",
    "        \n",
    "        self._lambda = np.random.normal(1, 0.01, (n_topics, maxlen_vocabs))\n",
    "        self._eq_lnbeta = self._dirichlet_expectation(self._lambda)\n",
    "        self._exp_eq_lnbeta = np.exp(self._eq_lnbeta)\n",
    "\n",
    "        \n",
    "    def _dirichlet_expectation(self, params):\n",
    "        \"\"\"\n",
    "        Input  : Parameter vector of Dirichlet distribution(np.array)\n",
    "        Output : Expectation of log of corresponding Dirichlet random vector(np.array)\n",
    "        \"\"\"\n",
    "        if len(params.shape) == 1:\n",
    "            return digamma(params) - digamma(np.sum(params))\n",
    "        return digamma(params) - digamma(np.sum(params, axis=1).reshape((-1,1)))\n",
    "    \n",
    "        \n",
    "    def _get_batch_indices(self):\n",
    "        \"\"\"\n",
    "        Operating np.random.choice(range(D), batchsize) becomes expensive when D gets larger.\n",
    "        This function provides faster way to generate random numbers from discrete uniform distribution [0, D)\n",
    "        \"\"\"\n",
    "        return list(map(int, np.random.random((self._S,)) * self._D))\n",
    "        \n",
    "        \n",
    "    def _reference_to_wordlist(self, reference):\n",
    "        \"\"\"\n",
    "        (** Function to be distributed **)\n",
    "        Process of converting : single <reference> - <document> - <wordlist>\n",
    "\n",
    "         1. reference -> document (** domain specific **)\n",
    "             In this simple example, this task is simple because reference is document by itself. \n",
    "             However, this job can be more complicated if this is not the case.\n",
    "             (ex. reference can be only a URL whose corresponding page contains body text to be scraped.)\n",
    "\n",
    "         2. document -> wordlist (** domain specific **)\n",
    "             This example extracts words from English document, so textblob.TextBlob is used.\n",
    "             Depending on the specific language to be analyzed, morpheme analyzer should be changed accordingly.\n",
    "        \"\"\"\n",
    "        document = reference\n",
    "\n",
    "        wordlist = [tag[0].lower() \n",
    "                    for tag in TextBlob(document).tags \n",
    "                    if tag[1].startswith('NN') and len(tag[0]) > 2]\n",
    "        \n",
    "        return wordlist\n",
    "    \n",
    "    \n",
    "    def _wordlists_to_indexcounts(self, wordlists):\n",
    "        \"\"\"\n",
    "        Convert <wordlists> into 2-dim array of word indices and corresponding count, denoted as <indexcount>. \n",
    "        Index of a word is referred from vocab2idx(defaultdict) defined at the initialization of the instance.\n",
    "        \"\"\"\n",
    "        indexcounts = []\n",
    "        for wordlist in wordlists:\n",
    "            indexcount = Counter([self._vocab2idx[word] for word in wordlist])\n",
    "            index = list(indexcount.keys())\n",
    "            count = list(indexcount.values())\n",
    "            indexcounts.append([index, count])\n",
    "            \n",
    "        return indexcounts\n",
    "        \n",
    "    \n",
    "    def _update_locals(self, indexcounts):\n",
    "        \"\"\"\n",
    "        Update local variational parameters corresponding to documents in a minibatch(indexcounts).\n",
    "        \n",
    "        Notation\n",
    "         - exp_eq_lnbw  : \\mathbb{E}_q[\\ln\\beta_{.w_s}] (array of shape K * len(indices))\n",
    "         - eq_lntheta_s : \\mathbb{E}_q[\\ln\\theta_s]\n",
    "         - exp_eq_lnths : \\exp\\{\\mathbb{E}_q[\\ln\\theta_s]\\}\n",
    "         - phis_normalizers : \\sum_{k=1}^{K}_\\phi_{s}^k\n",
    "         \n",
    "        Process\n",
    "        - To make Dirichlet prior as flat as possible, assign value close to 1 as possible when initializing gamma\n",
    "         1. Initialize gamma_s, s = 1, ... ,S\n",
    "         2. for each indexcount_s:\n",
    "          2-1. Calculate expectation on initialized values\n",
    "          2-2. Update gamma_s until its change is ignorable\n",
    "          2-3. Record gamma_s and update sufficient statistic \n",
    "        \"\"\"        \n",
    "        gamma = np.random.normal(1., 0.01, (self._S, self._K))\n",
    "        suffstats = np.zeros(self._lambda.shape)\n",
    "        \n",
    "        for s, indexcount in enumerate(indexcounts):\n",
    "            indicies, counts = indexcount\n",
    "            exp_eq_lnbw = self._exp_eq_lnbeta[:,indices]\n",
    "            gamma_s = gamma[s,:]        \n",
    "            eq_lntheta_s = self._dirichlet_expectation(gamma_s)\n",
    "            exp_eq_lnths = np.exp(eq_lntheta_s)\n",
    "            phis_normalizers = np.dot(exp_eq_lnths, exp_eq_lnbw) + 1e-100\n",
    "            \n",
    "            for _ in range(self._local_maxiter):\n",
    "                gamma_s_old = gamma_s.copy()\n",
    "                gamma_s = self._alpha + exp_eq_lnths * np.dot(counts / phis_normalizers, exp_eq_lnbw.T)\n",
    "                eq_lntheta_s = self._dirichlet_expectation(gamma_s)\n",
    "                exp_eq_lnths = np.exp(eq_lntheta_s)\n",
    "                phis_normalizers = np.dot(exp_eq_lnths, exp_eq_lnbw) + 1e-100\n",
    "                if np.mean(np.abs(gamma_s - gamma_s_old)) < tol:\n",
    "                    break\n",
    "                    \n",
    "            gamma[s,:] = gamma_s\n",
    "            suffstats[:, indicies] += np.outer(exp_eq_lnths.T, counts/phis_normalizers)\n",
    "            \n",
    "        suffstats *= self._exp_eq_lnbeta\n",
    "        return gamma, suffstats\n",
    "    \n",
    "    \n",
    "    def _update_global(self, indexcounts, t):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        gamma, suffstats = self._update_local(indexcounts)\n",
    "        \n",
    "        return gamma\n",
    "    \n",
    "    \n",
    "    def _calculate_elbo(self, indexcounts, gamma):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def batch_call(self):\n",
    "        \"\"\"\n",
    "         1. Sample subset of size S from references\n",
    "         2. Convert each reference into wordlists\n",
    "         3. return corresponding indexcounts.\n",
    "        \"\"\"\n",
    "        batch_idx = self._get_batch_indices()\n",
    "        minibatch = self.references[batch_idx]\n",
    "\n",
    "        wordlists = []\n",
    "        for reference in minibatch:\n",
    "            wordlists.append(self._reference_to_wordlist(reference))\n",
    "        \n",
    "        return self._wordlists_to_indexcounts(wordlists)\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.ELBO = [-np.inf]\n",
    "        \n",
    "        for t in range(self._global_maxiter):\n",
    "            indexcounts = self.batch_call()\n",
    "            gamma = self._update_global(indexcounts, t)\n",
    "            self.ELBO.append(self._calculate_elbo(indexcounts, gamma))\n",
    "            \n",
    "    \n",
    "    def idx2vocab(self):\n",
    "        \"\"\"\n",
    "        Create hashmap from each vocaulary's index to vocabulary for result interpretation\n",
    "        \"\"\"\n",
    "        return dict((item[1], item[0]) for item in self._vocab2idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dirichlet_expectation(params):\n",
    "    \"\"\"\n",
    "    Input  : Parameter vector of Dirichlet distribution(np.array)\n",
    "    Output : Expectation of log of corresponding Dirichlet random vector(np.array)\n",
    "    \"\"\"\n",
    "    if len(params.shape) == 1:\n",
    "        return digamma(params) - digamma(np.sum(params))\n",
    "    return digamma(params) - digamma(np.sum(params, axis=1).reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_local(indices, counts)\n",
    "    \"\"\"\n",
    "    (** Function to be distributed **)\n",
    "    Update local variational parameter for single document in a minibatch.\n",
    "\n",
    "    Notatation\n",
    "     - eq_lnthd     : \\mathbb{E}_q[\\ln\\theta_d]\n",
    "     - exp_eq_lnthd : \\exp\\{\\mathbb{E}_q[\\ln\\theta_d]\\}\n",
    "     - exp_eq_lnbw  : \\mathbb{E}_q[\\ln\\beta_w] (array of shape K * len(indices))\n",
    "     - sum_k_phidn  : \\sum_{k=1}^{K}_\\phi_{dn}^k\n",
    "    \"\"\"\n",
    "    gamma_d = np.random.normal(1., 0.01, (self._K,))\n",
    "    eq_lnthd = self._dirichlet_expectation(gamma_d)\n",
    "    exp_eq_lnthd = np.exp(eq_lnthd)\n",
    "    exp_eq_lnbw = self._exp_eq_lnbeta[:, indices]\n",
    "    sum_k_phidn = np.dot(exp_eq_lnthd, exp_eq_lnbw) + 1e-100\n",
    "\n",
    "    for _ in range(self._local_maxiter):\n",
    "        gamma_d_old = gamma_d.copy()\n",
    "        gamma_d = self._alpha + exp_eq_lnthd * np.dot(counts / sum_k_phidn, exp_eq_lnbw.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_eq_lnthd = np.exp(_dirichlet_expectation(np.random.normal(1., 0.01, (3,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_eq_lnbw = _dirichlet_expectation(np.random.normal(1., 0.01, (3,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23019205, 0.22146245, 0.21920042])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_eq_lnthd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.06926277, -2.06123602, -2.09115928],\n",
       "       [-2.06444983, -2.10291524, -2.09700874],\n",
       "       [-2.05158881, -2.10750919, -2.05389508],\n",
       "       [-2.11329267, -2.08140144, -2.08409843],\n",
       "       [-2.10212964, -2.08978762, -2.11048065]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_eq_lnbw.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OnlineLDA(korea, len(korea), 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,0,-1],[-1,0,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty((2,0)).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3., 4.],\n",
       "       [2., 3., 4.]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((np.empty((2,0)), np.array([[2.,3., 4.],[2.,3., 4.]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dirichlet_expectation : 파라미터 벡터를 받아 각 원소에 대해 기대값 계산한 벡터 반환\n",
    "parse_doc_list : 문서들의 리스트, word2idx를 받아 각 문서별로 id, count를 리스트로 담아 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def alalala(a,\n",
    "           b,\n",
    "           c,\n",
    "           d):\n",
    "    return a+b+c+d\n",
    "\n",
    "alalala(1,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-17 15:43:03,542\tERROR worker.py:666 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.880129814147949"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "result = ray.get(container)\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1, 1], [0, 2], [1, 3], [2, 4], [3, 5]]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
