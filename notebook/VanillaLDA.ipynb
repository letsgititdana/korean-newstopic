{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.special import digamma\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikitext_sentences(url):\n",
    "    response = requests.get(url)\n",
    "    parsed = BeautifulSoup(response.text, 'html.parser')\n",
    "    return [tag.get_text().strip() for tag in parsed.select('div.mw-parser-output')[0].find_all('p')]\n",
    "\n",
    "\n",
    "def postprocess(paragraphs):\n",
    "    result = []\n",
    "    wrapper = re.compile('[({\\[].{,20}[)}\\]]')\n",
    "    whitespace = re.compile('\\s')\n",
    "    for paragraph in paragraphs:\n",
    "        processed = wrapper.sub('', paragraph)\n",
    "        result.append(whitespace.sub(' ', processed))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 말뭉치 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세 가지 서로 다른 주제의 wikipedia ariticle들의 각 문단들을 하나의 document로 간주해서 가장 기본 형태의 LDA를 구현한 후 이 모형이 세 가지 토픽의 구성 단어들을 효과적으로 찾아내는지 확인한다. 세 article들의 제목은 각각 아래와 같다. 간단한 전처리를 위해 모든 소괄호, 대괄호와 그 안에 포함된 문자열을 삭제했고, `\\n`과 같이 공백을 표현하는 문자열은 ' '로 일괄 변경했다.\n",
    "\n",
    "1. Korea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korea is a region in East Asia; since 1945 it has been divided into what are now two distinct sovereign states: North Korea (officially the \"Democratic People's Republic of Korea\") and South Korea (officially the \"Republic of Korea\"). Korea consists of the Korean Peninsula, Jeju Island, and several minor islands near the peninsula. It is bordered by China to the northwest and Russia to the northeast. It is separated from Japan to the east by the Korea Strait and the Sea of Japan .\n"
     ]
    }
   ],
   "source": [
    "korea = postprocess(get_wikitext_sentences(\"https://en.wikipedia.org/wiki/Korea\"))\n",
    "print(korea[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Aristotle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aristotle  Greek: Ἀριστοτέλης Aristotélēs, pronounced ; 384–322 BC) was a Greek philosopher and polymath during the Classical period in Ancient Greece. Taught by Plato, he was the founder of the Lyceum, the Peripatetic school of philosophy, and the Aristotelian tradition. His writings cover many subjects including physics, biology, zoology, metaphysics, logic, ethics, aesthetics, poetry, theatre, music, rhetoric, psychology, linguistics, economics, politics, and government. Aristotle provided a complex synthesis of the various philosophies existing prior to him. It was above all from his teachings that the West inherited its intellectual lexicon, as well as problems and methods of inquiry. As a result, his philosophy has exerted a unique influence on almost every form of knowledge in the West and it continues to be a subject of contemporary philosophical discussion.\n"
     ]
    }
   ],
   "source": [
    "aristotle = postprocess(get_wikitext_sentences(\"https://en.wikipedia.org/wiki/Aristotle\"))\n",
    "print(aristotle[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google, LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, a search engine, cloud computing, software, and hardware. It is considered one of the Big Four technology companies, alongside Amazon, Apple, and Microsoft.\n"
     ]
    }
   ],
   "source": [
    "google = postprocess(get_wikitext_sentences(\"https://en.wikipedia.org/wiki/Google\"))\n",
    "print(google[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세 부류의 문단들을 하나로 합쳐서 말뭉치를 만들고, 문단들의 순서를 랜덤하게 섞는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = korea + aristotle + google\n",
    "documents = list(np.random.choice(documents, len(documents), replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모형 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 322개의 문서가 있고, 한 iteration마다 전체 문서에서 batchsize만큼의 문서를 샘플링해서 variational parameter들을 업데이트한다. batchsize가 총 문서의 개수와 같으면(즉, 이 예시에서는 322면) 일반적인 LDA와 같은 결과를 얻을 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineLDA:\n",
    "    \n",
    "    def __init__(self, corpus, n_documents, n_topics):\n",
    "        self.word2idx = defaultdict(lambda: len(self.word2idx))\n",
    "        self.D = n_documents\n",
    "        self.K = n_topic\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns(document):\n",
    "    \"\"\"\n",
    "    Return the list of extracted nouns from input document using hardcoded morpheme analyzer\n",
    "    (** domain specific : morpheme analyzer **)\n",
    "    \n",
    "    input\n",
    "     - document : str\n",
    "     \n",
    "    output\n",
    "     - nouns found in the input document : list\n",
    "    \"\"\"\n",
    "    return [tag[0] for tag in TextBlob(document).tags if tag[1].startswith('NN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Proof for randomness of extract_nouns function\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# upper_limit = 10\n",
    "# size = 10000\n",
    "\n",
    "# indices = get_batch_indices(size, upper_limit) # generate indices from discrete random uniform of specified size\n",
    "# result = sorted(Counter(indices).items(), key=lambda x: x[0])\n",
    "# plt.bar([pair[0] for pair in result], [pair[1] for pair in result])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_indexset(document, word2idx):\n",
    "    \"\"\"\n",
    "    Convert list of nouns into list of indices using the word-index map\n",
    "    \n",
    "    input\n",
    "     - document : str\n",
    "     - word2idx : defaultdict\n",
    "     \n",
    "    output\n",
    "     - list of indices converted from list of nouns : list\n",
    "    \"\"\"\n",
    "    nouns = extract_nouns(document)\n",
    "    return [word2idx[word] for word in nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_indices(size, D):\n",
    "    \"\"\"\n",
    "    Faster way to generate random numbers from discrete uniform distribution [0, D)\n",
    "    \n",
    "    input\n",
    "     - size : int\n",
    "     - U : int\n",
    "    \n",
    "    output\n",
    "     - list of random numbers\n",
    "    \"\"\"\n",
    "    return list(map(int, np.random.random((size,)) * D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_call(reference, D, batchsize, word2idx):\n",
    "    \"\"\"\n",
    "    From corpus or object which can be converted into corpus, do:\n",
    "     1. select mini-batch\n",
    "     2. convert this into indexset\n",
    "    (** domain specific : reference is exhaustive list of paragraphs in this example, but it could be just list of URLs to be scraped **)\n",
    "    \n",
    "    input\n",
    "     - reference : list\n",
    "     - D, batchsize : int\n",
    "    \n",
    "    output\n",
    "     - coordinates : (document index, word index) of documents in the batch\n",
    "    \"\"\"\n",
    "    coordinates = []\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
