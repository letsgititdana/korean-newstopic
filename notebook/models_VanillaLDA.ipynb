{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import digamma\n",
    "from textblob import TextBlob\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaLDA:\n",
    "    \"\"\"\n",
    "    Class to apply LDA to toy corpus to make sense of LDA mechanisms.\n",
    "    \n",
    "    Assumptions on toy corpus : Size of the corpus is small enough so that \n",
    "     1. counting the exact number of vocabularies in corpus is easy\n",
    "     2. corpus can be expressed in dense array\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_docs, n_vocabs, n_topics, maxiter=500, tol=1e-4):\n",
    "        \"\"\"\n",
    "        lambda, gamma are array of variational parameters of topic, topic proportion respectively.\n",
    "         - lam : K * V array whose row refers to k-th topic\n",
    "         - gam : D * K array whose row refers to topic proportion of d-th document\n",
    "        \"\"\"\n",
    "        self.D = n_docs\n",
    "        self.V = n_vocabs\n",
    "        self.K = n_topics\n",
    "        self.lam = np.ones((n_topics, n_vocabs))\n",
    "        self.gam = np.ones((n_docs, n_topics))\n",
    "        self.maxiter = maxiter\n",
    "        self.tol = tol\n",
    "    \n",
    "    \n",
    "    def fit(self, corpus, alpha=1., eta=1.):\n",
    "        \"\"\"\n",
    "        Apply LDA to toy corpus.\n",
    "         - corpus : 2-dim np.array of coordinates which refer to the count of vocabulary\n",
    "         - alpha, eta : hyperparameters related to topic proportion and topic respectively\n",
    "         \n",
    "        Updating process iterates until Frobenius norm between both new and old lambda, gamma converges.\n",
    "        As size of the corpus is assumed to be small enough, presented operation is really coarse.\n",
    "        \"\"\"\n",
    "        phi = np.zeros((self.K, self.D, self.V))\n",
    "        \n",
    "        for iteration in range(self.maxiter):\n",
    "            \n",
    "            lam_old = self.lam.copy()\n",
    "            gam_old = self.gam.copy()\n",
    "\n",
    "            # phi\n",
    "            digamma_lam = (digamma(self.lam).T - digamma(np.apply_along_axis(np.sum, 1, self.lam))).T\n",
    "            digamma_gam = (digamma(self.gam).T - digamma(np.apply_along_axis(np.sum, 1, self.gam))).T\n",
    "            for d in range(self.D):\n",
    "                for v in range(self.V):\n",
    "                    ln_phi_dv = digamma_gam[:,d] + digamma_lam[:,v]\n",
    "                    phi[:,d,v] = np.exp(ln_phi_dv) / np.sum(np.exp(ln_phi_dv))\n",
    "            \n",
    "            # gamma\n",
    "            for d in range(self.D):\n",
    "                self.gam[d,:] = np.apply_along_axis(np.sum, 1, phi[:,d,:].T * corpus[d,:])\n",
    "            \n",
    "            # lambda\n",
    "            for k in range(self.K):\n",
    "                self.lam[k,:] = np.apply_along_axis(np.sum, 1, corpus * phi[k,:,:])\n",
    "                \n",
    "            # Convergence\n",
    "            bool1 = np.linalg.norm(lam_old - self.lam) < self.tol\n",
    "            bool2 = np.linalg.norm(gam_old - self.gam) < self.tol\n",
    "            if bool1 and bool2:\n",
    "                break\n",
    "                \n",
    "        if (iteration+1) == self.maxiter:\n",
    "            print(\"Algorithm did not converge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generating process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, generate toy data to test the performance of the implemented algorithm. For simplicity, collect nouns from 3 distinct wikipedia articles. Title of each article was\n",
    "1. Bayes' theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes = [\n",
    "    \"In probability theory and statistics, Bayes's theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\",\n",
    "    \"For example, if the risk of developing health problems is known to increase with age, Bayes's theorem allows the risk to an individual of a known age to be assessed more accurately than simply assuming that the individual is typical of the population as a whole.\",\n",
    "    \"One of the many applications of Bayes's theorem is Bayesian inference, a particular approach to statistical inference.\",\n",
    "    \"When applied, the probabilities involved in Bayes's theorem may have different probability interpretations.\",\n",
    "    \"With Bayesian probability interpretation, the theorem expresses how a degree of belief, expressed as a probability, should rationally change to account for the availability of related evidence.\",\n",
    "    \"Bayesian inference is fundamental to Bayesian statistics.\",\n",
    "    \"Bayes's theorem is named after Reverend Thomas Bayes, who first used conditional probability to provide an algorithm (his Proposition 9) that uses evidence to calculate limits on an unknown parameter, published as An Essay towards solving a Problem in the Doctrine of Chances (1763). \",\n",
    "    \"In what he called a scholium, Bayes extended his algorithm to any unknown prior cause.\",\n",
    "    \"Independently of Bayes, Pierre-Simon Laplace in 1774, and later in his 1812 Théorie analytique des probabilités, used conditional probability to formulate the relation of an updated posterior probability from a prior probability, given evidence.\",\n",
    "    \"Sir Harold Jeffreys put Bayes's algorithm and Laplace’s formulation on an axiomatic basis, writing that Bayes's theorem 'is to the theory of probability what the Pythagorean theorem is to geometry'.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Guinness Brewery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "guinness = [\n",
    "    \"St. James's Gate Brewery (Irish: Grúdlann Gheata Naomh Séamuis) is a brewery founded in 1759 in Dublin, Ireland, by Arthur Guinness.\",\n",
    "    \"The company is now a part of Diageo, a British company formed from the merger of Guinness and Grand Metropolitan in 1997.\",\n",
    "    \"The main product of the brewery is Guinness Draught.\",\n",
    "    \"Originally leased in 1759 to Arthur Guinness at 45pound per year for 9,000 years, the St. James's Gate area has been the home of Guinness ever since.\",\n",
    "    \"It became the largest brewery in Ireland in 1838, and the largest in the world by 1886, with an annual output of 1.2 million barrels.\",\n",
    "    \"Although no longer the largest brewery in the world, it remains as the largest brewer of stout.\",\n",
    "    \"The company has since bought out the originally leased property, and during the 19th and early 20th centuries the brewery owned most of the buildings in the surrounding area, including many streets of housing for brewery employees, and offices associated with the brewery.\",\n",
    "    \"The brewery had its own power plant.\",\n",
    "    \"There is an attached exhibition on the 250-year-old history of Guinness, called the Guinness Storehouse.\",\n",
    "    \"Arthur Guinness started brewing ales in Leixlip, County Kildare, and then from 1759 at the St. James's Gate Brewery in Dublin.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "google = [\n",
    "    'Google, LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, a search engine, cloud computing, software, and hardware',\n",
    "    'It is considered one of the Big Four technology companies, alongside Amazon, Apple, and Microsoft',\n",
    "    'Google was founded in September 1998 by Larry Page and Sergey Brin while they were Ph.D students at Stanford University in California',\n",
    "    'Together they own about 14 percent of its shares and control 56 percent of the stockholder voting power through supervoting stock',\n",
    "    'They incorporated Google as a California privately held company on September 4, 1998, in California',\n",
    "    'Google was then reincorporated in Delaware on October 22, 2002',\n",
    "    'An initial public offering (IPO) took place on August 19, 2004, and Google moved to its headquarters in Mountain View, California, nicknamed the Googleplex',\n",
    "    'In August 2015, Google announced plans to reorganize its various interests as a conglomerate called Alphabet Inc',\n",
    "    \"Google is Alphabet's leading subsidiary and will continue to be the umbrella company for Alphabet's Internet interests\",\n",
    "    'Sundar Pichai was appointed CEO of Google, replacing Larry Page who became the CEO of Alphabet.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then extract nouns from each group of sentences. Class `TextBlob` defined in `textblob` package will be used to tag words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns(document):\n",
    "    return [tag[0] for tag in TextBlob(document).tags if tag[1].startswith('NN')]\n",
    "\n",
    "nouns = {\n",
    "    'bayes' : [],\n",
    "    'guinness' : [],\n",
    "    'google' : []\n",
    "}\n",
    "\n",
    "index = 0\n",
    "label = ['bayes', 'guinness', 'google']\n",
    "\n",
    "for category in [bayes, guinness, google]:\n",
    "    for document in category:\n",
    "        nouns[label[index]] += extract_nouns(document)\n",
    "    nouns[label[index]] = set(nouns[label[index]])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `guinness` and `google` share two words in common, remove them from `google` for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company', 'power'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns['guinness'].intersection(nouns['google'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns['google'] = nouns['google'].difference({'company', 'power'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then reconstruct this sets as single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(nouns['bayes']) + list(nouns['guinness']) + list(nouns['google'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_bayes = len(nouns['bayes'])\n",
    "len_guinness = len(nouns['guinness'])\n",
    "len_google = len(nouns['google'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create hypothetical composition of words from each of these category. Simply assume that occurence frequency of each word in each category is equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_bayes = [1/len_bayes for i in range(len_bayes)] + [0 for i in range(len_guinness)] + [0 for i in range(len_google)]\n",
    "topic_guinness = [0 for i in range(len_bayes)] + [1/len_guinness for i in range(len_guinness)] + [0 for i in range(len_google)]\n",
    "topic_google = [0 for i in range(len_bayes)] + [0 for i in range(len_guinness)] + [1/len_google for i in range(len_google)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example they can be thought of as article of Google employee who majored Bayesian Statistics and loves Guiness beer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, define map from word to corresponding index of words and format each documents as coordinates of counts of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = defaultdict(lambda: len(word2idx))\n",
    "indices = []\n",
    "for d in range(len(documents)):\n",
    "    words = extract_nouns(documents[d])\n",
    "    coordinates = [(d, word2idx[word], count) for word, count in Counter(extract_nouns(documents[d])).items()]\n",
    "    indices.extend(coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the coordinates, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.zeros((len(documents), len(word2idx)))\n",
    "for index in indices:\n",
    "    corpus[index[0], index[1]] = index[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (30,) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-94dbab4dd0ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVanillaLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-9f55d06c49a2>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, corpus, alpha, eta)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     \u001b[0mln_phi_dv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdigamma_gam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdigamma_lam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                     \u001b[0mphi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mln_phi_dv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mln_phi_dv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (30,) (3,) "
     ]
    }
   ],
   "source": [
    "model = VanillaLDA(len(documents), len(word2idx), 3)\n",
    "model.fit(corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
